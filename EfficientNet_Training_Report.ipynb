{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Music Genre Classification using Transfer Learning (EfficientNetB0)\n",
                "\n",
                "**Course Project: Advanced Neural Networks**  \n",
                "**Model Architecture:** EfficientNetB0 (Compound Scaling)  \n",
                "**Dataset:** GTZAN Genre Collection  \n",
                "**Dataset Link:** [Kaggle: GTZAN Dataset](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification/data)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Introduction\n",
                "This notebook demonstrates a high-precision approach to music genre classification. We leverage **Transfer Learning** by utilizing a pre-trained **EfficientNetB0** model, which has been fine-tuned on Mel-Spectrograms derived from audio samples. \n",
                "\n",
                "### Key Technical Features:\n",
                "- **Compound Scaling:** Balanced adjustment of network depth, width, and resolution.\n",
                "- **Audio Slicing:** Augmenting data by dividing 30-second tracks into smaller segments.\n",
                "- **Mel-Spectrograms:** Visualizing audio frequency maps to allow CNNs to process sound as \"images\"."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration & Dependencies\n",
                "We define our hyper-parameters and environment constants, ensuring a fixed input shape for the EfficientNet backbone."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import zipfile\n",
                "import numpy as np\n",
                "import librosa\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.applications import EfficientNetB0\n",
                "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Rescaling\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import traceback\n",
                "import gc\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "SAMPLE_RATE = 22050\n",
                "DURATION = 30\n",
                "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
                "INPUT_SHAPE = (224, 224, 3)\n",
                "NUM_CLASSES = 10\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 15\n",
                "SLICES_PER_TRACK = 10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dataset Extraction\n",
                "We perform a robust extraction of the GTZAN dataset, ensuring the directory structure is correctly mapped even in cloud environments like Google Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ZIP_PATH = '/content/genres_original.zip'\n",
                "EXTRACT_PATH = '/content/dataset_extracted'\n",
                "\n",
                "if not os.path.exists(EXTRACT_PATH):\n",
                "    if os.path.exists(ZIP_PATH):\n",
                "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
                "            zip_ref.extractall(EXTRACT_PATH)\n",
                "        print(\"Unzip complete.\")\n",
                "    else:\n",
                "        print(\"Dataset zip not found! Ensure it is uploaded to /content/.\")\n",
                "\n",
                "DATASET_PATH = EXTRACT_PATH\n",
                "for root, dirs, files in os.walk(EXTRACT_PATH):\n",
                "    if 'blues' in dirs:\n",
                "        DATASET_PATH = root\n",
                "        break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Exploratory Data Analysis (EDA)\n",
                "In this section, we explore the GTZAN dataset by analyzing a single audio track. We visualize its waveform and demonstrate the **3-second slicing logic** used during training to augment our dataset from 1,000 to 10,000 samples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import librosa\n",
                "import librosa.display\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import IPython.display as ipd\n",
                "import os\n",
                "import cv2\n",
                "\n",
                "# Search for a sample file\n",
                "sample_file = 'genres_original/blues/blues.00000.wav' # Local path\n",
                "if not os.path.exists(sample_file):\n",
                "    # Try looking in Colab extracted path if it exists\n",
                "    colab_path = '/content/dataset_extracted/genres_original/blues/blues.00000.wav'\n",
                "    if os.path.exists(colab_path): sample_file = colab_path\n",
                "\n",
                "if os.path.exists(sample_file):\n",
                "    # 1. Load Audio\n",
                "    y, sr = librosa.load(sample_file, sr=22050)\n",
                "    \n",
                "    # 2. Plot Full Waveform\n",
                "    plt.figure(figsize=(14, 5))\n",
                "    librosa.display.waveshow(y, sr=sr, color='blue')\n",
                "    plt.title('Full 30-Second Waveform (Blues)')\n",
                "    plt.xlabel('Time (s)')\n",
                "    plt.ylabel('Amplitude')\n",
                "    plt.show()\n",
                "    \n",
                "    # Play the audio\n",
                "    print(\"Playing Sample Audio:\")\n",
                "    ipd.display(ipd.Audio(sample_file))\n",
                "else:\n",
                "    print('Sample file not found. Please ensure dataset is extracted.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Demonstrating 3-Second Slicing & Mel-Spectrogram Extraction\n",
                "To maximize the learning capacity of our model, we slice each 30-second track into ten 3-second segments. Each segment is then converted into a **Mel-Spectrogram**, which represents the intensity of frequencies over time on a log scale (Mel scale), mimicking human hearing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if os.path.exists(sample_file):\n",
                "    # Define slicing parameters\n",
                "    duration = 30\n",
                "    slices_per_track = 10\n",
                "    slice_duration = duration / slices_per_track\n",
                "    samples_per_slice = int(slice_duration * sr)\n",
                "\n",
                "    plt.figure(figsize=(18, 8))\n",
                "    plt.suptitle('Data Augmentation: 3-Second Mel-Spectrogram Slices', fontsize=16)\n",
                "\n",
                "    # Visualize specific slices (e.g., first 4 slices)\n",
                "    for i in range(4):\n",
                "        start_sample = i * samples_per_slice\n",
                "        end_sample = start_sample + samples_per_slice\n",
                "        chunk = y[start_sample:end_sample]\n",
                "        \n",
                "        # Generate Mel-Spectrogram\n",
                "        mel = librosa.power_to_db(librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=128))\n",
                "        \n",
                "        plt.subplot(2, 2, i + 1)\n",
                "        librosa.display.specshow(mel, sr=sr, x_axis='time', y_axis='mel')\n",
                "        plt.title(f'Slice {i+1} (Seconds {i*3}-{(i+1)*3})')\n",
                "        plt.colorbar(format='%+2.0f dB')\n",
                "\n",
                "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. RAM-Optimized Data Loading & Preprocessing\n",
                "Efficiency is critical when handling spectrogram data. We pre-allocate memory using `uint8` to save RAM and use `librosa` for high-quality Mel-Spectrogram extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data_sliced_optimized(dataset_path, slices_per_track=10):\n",
                "    genres = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])\n",
                "    genre_to_id = {genre: i for i, genre in enumerate(genres)}\n",
                "    total_files = sum([len([f for f in os.listdir(os.path.join(dataset_path, g)) if f.endswith('.wav')]) for g in genres])\n",
                "    total_slices = total_files * slices_per_track\n",
                "\n",
                "    X = np.zeros((total_slices, 224, 224, 3), dtype=np.uint8)\n",
                "    y = np.zeros((total_slices,), dtype=np.int32)\n",
                "\n",
                "    SAMPLES_PER_SLICE = int(SAMPLES_PER_TRACK / slices_per_track)\n",
                "    current_idx = 0\n",
                "\n",
                "    for genre in genres:\n",
                "        genre_path = os.path.join(dataset_path, genre)\n",
                "        for filename in os.listdir(genre_path):\n",
                "            if filename.endswith('.wav'):\n",
                "                try:\n",
                "                    signal, sr = librosa.load(os.path.join(genre_path, filename), sr=SAMPLE_RATE, duration=DURATION)\n",
                "                    for s in range(slices_per_track):\n",
                "                        start, end = s * SAMPLES_PER_SLICE, (s+1) * SAMPLES_PER_SLICE\n",
                "                        mel = librosa.power_to_db(librosa.feature.melspectrogram(y=signal[start:end], sr=sr, n_mels=128))\n",
                "                        resized = cv2.resize(mel, (224, 224))\n",
                "                        rgb = np.stack([resized] * 3, axis=-1)\n",
                "                        min_v, max_v = np.min(rgb), np.max(rgb)\n",
                "                        norm = (rgb - min_v) / (max_v - min_v) * 255.0 if max_v > min_v else np.zeros_like(rgb)\n",
                "                        X[current_idx] = norm.astype(np.uint8)\n",
                "                        y[current_idx] = genre_to_id[genre]\n",
                "                        current_idx += 1\n",
                "                except: continue\n",
                "        gc.collect()\n",
                "    return X[:current_idx], y[:current_idx], genres"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Architecture (Transfer Learning)\n",
                "We load the EfficientNetB0 backbone with weights pre-trained on ImageNet. Critical architectural decisions include:\n",
                "- **Freezing BatchNormalization:** Prevents the noise statistics from being corrupted during fine-tuning.\n",
                "- **Custom Head:** Includes a GlobalAveragePooling2D layer, a dense layer for non-linear mapping, and a Dropout layer to reduce overfitting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_fixed_model(input_shape, num_classes):\n",
                "    inputs = tf.keras.Input(shape=input_shape)\n",
                "    x = Rescaling(1.0)(inputs)\n",
                "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_tensor=x)\n",
                "    base_model.trainable = True\n",
                "    for layer in base_model.layers:\n",
                "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
                "            layer.trainable = False\n",
                "    \n",
                "    x = GlobalAveragePooling2D()(base_model.output)\n",
                "    x = Dense(512, activation='relu')(x)\n",
                "    x = Dropout(0.5)(x)\n",
                "    outputs = Dense(num_classes, activation='softmax')(x)\n",
                "    \n",
                "    model = Model(inputs=inputs, outputs=outputs)\n",
                "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training Visualization Logic\n",
                "Utilities for monitoring the learning process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_history(history):\n",
                "    plt.figure(figsize=(12, 4))\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.plot(history.history['accuracy'], label='Train')\n",
                "    plt.plot(history.history['val_accuracy'], label='Val')\n",
                "    plt.title('Accuracy'); plt.legend()\n",
                "    \n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.plot(history.history['loss'], label='Train')\n",
                "    plt.plot(history.history['val_loss'], label='Val')\n",
                "    plt.title('Loss'); plt.legend()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Model Execution\n",
                "Loading data, splitting into Train/Test sets, and running the training loop for 15 epochs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y, genres = load_data_sliced_optimized(DATASET_PATH, slices_per_track=SLICES_PER_TRACK)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "model = create_fixed_model(INPUT_SHAPE, NUM_CLASSES)\n",
                "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
                "                    epochs=EPOCHS, batch_size=BATCH_SIZE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Final Evaluation & Export\n",
                "Displaying the classification report and saving the model in multiple formats for deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
                "print(classification_report(y_test, y_pred, target_names=genres))\n",
                "plot_history(history)\n",
                "\n",
                "model.save(\"EfficientNet_Model.keras\")\n",
                "model.save(\"EfficientNet_Model.h5\", include_optimizer=False)\n",
                "model.save_weights(\"EfficientNet_Model.weights.h5\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}